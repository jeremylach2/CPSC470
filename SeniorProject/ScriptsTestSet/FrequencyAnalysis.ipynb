{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PreprocessLib.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb #allows access to import other notebooks\n",
    "from PreprocessLib import cleanText, removeSingleLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#processes file for frequency analysis, optionally tokenizes\n",
    "def process_file(a_file, tokenize=True):\n",
    "    string_without_line_breaks = \"\"\n",
    "    for line in a_file:\n",
    "        stripped_line = line.rstrip() + \" \"\n",
    "        string_without_line_breaks += stripped_line\n",
    "    a_file.close()\n",
    "    sentences = string_without_line_breaks\n",
    "    \n",
    "    \n",
    "   \n",
    "    if (tokenize == False):\n",
    "        cleanedFile = cleanText(sentences, tokenize)\n",
    "        return cleanedFile\n",
    "    cleanedFile = cleanText(sentences) #cleanText is found in PreprocessLib.ipynb\n",
    "        \n",
    "    output_doc = []\n",
    "    nlp = spacy.load(\"el_core_news_sm\")\n",
    "\n",
    "    for sentence in cleanedFile:\n",
    "        temp = []\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            temp.append(token.text)\n",
    "        output_doc.append(temp)\n",
    "        output_doc = removeSingleLists(output_doc)\n",
    "    return output_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns file in string format\n",
    "def stringify(a_file):\n",
    "    string_without_line_breaks = \"\"\n",
    "    for line in a_file:\n",
    "        stripped_line = line.rstrip() + \" \"\n",
    "        string_without_line_breaks += stripped_line\n",
    "    a_file.close()\n",
    "    sentences = string_without_line_breaks\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "longFile = open(\"./other-sources/Longus.txt\", \"r\", encoding=\"utf8\")\n",
    "markFile = open(\"./other-sources/Mark.txt\", \"r\", encoding=\"utf8\")\n",
    "mathFile = open(\"./other-sources/Mathew.txt\", \"r\", encoding=\"utf8\")\n",
    "ignatFile = open(\"./other-sources/Ignatius-4.txt\", \"r\", encoding=\"utf8\")\n",
    "testFile = open(\"./other-sources/Ignatius-3books.txt\", \"r\", encoding=\"utf8\")\n",
    "longFreq = process_file(longFile, False)\n",
    "markFreq = process_file(markFile, False)\n",
    "mathFreq = process_file(mathFile, False)\n",
    "ignatFreq = process_file(ignatFile, False)\n",
    "testFreq = process_file(testFile, False)\n",
    "\n",
    "longFile = open(\"./other-sources/Longus.txt\", \"r\", encoding=\"utf8\")\n",
    "markFile = open(\"./other-sources/Mark.txt\", \"r\", encoding=\"utf8\")\n",
    "mathFile = open(\"./other-sources/Mathew.txt\", \"r\", encoding=\"utf8\")\n",
    "ignatFile = open(\"./other-sources/Ignatius-4.txt\", \"r\", encoding=\"utf8\")\n",
    "testFile = open(\"./other-sources/Ignatius-3books.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "#sentence tokenizes the documents\n",
    "long = stringify(longFile)\n",
    "mark = stringify(markFile)\n",
    "math = stringify(mathFile)\n",
    "ignat = stringify(ignatFile)\n",
    "test = stringify(testFile)\n",
    "from nltk import tokenize\n",
    "longSent = tokenize.sent_tokenize(long)\n",
    "markSent = tokenize.sent_tokenize(mark)\n",
    "mathSent = tokenize.sent_tokenize(math)\n",
    "ignatSent = tokenize.sent_tokenize(ignat)\n",
    "testSent = tokenize.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19826\n",
      "11282\n",
      "18349\n",
      "3969\n"
     ]
    }
   ],
   "source": [
    "print(len(longFreq.split()))\n",
    "print(len(markFreq.split()))\n",
    "print(len(mathFreq.split()))\n",
    "print(len(ignatFreq.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses a g-test, useful for finding differences in proportions\n",
    "#returns signifcance of G-Test\n",
    "def isSignificant(obA, exA, obB, exB):\n",
    "    import numpy as np\n",
    "    from scipy.stats import chi2_contingency\n",
    "    obs = np.array([[obA, exA-obA], [obB, exB-obB]])\n",
    "    p = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
    "    pVal = p[1] \n",
    "    if (pVal < 0.05):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "#returns list of instances and count in sorted order from most to least common\n",
    "def mostCommon(instances):\n",
    "    return sorted(sorted(Counter(instances).items(), key=itemgetter(0)), key=itemgetter(1), reverse=True)\n",
    "\n",
    "#returns frequency of word in txt\n",
    "def getFreq(word, txt):\n",
    "    return txt.count(word)\n",
    "\n",
    "#analyzes frequency of the words in the document\n",
    "#returns most common words of each document and count of significantly different word frequencies\n",
    "# and each iteration counts which authors words was most similar, then returns the list\n",
    "def analyzeFreq():\n",
    "    from scipy import stats\n",
    "    from collections import Counter\n",
    "    longList = re.sub(\"[^\\w]\", \" \",  longFreq).split()\n",
    "    longCounts = Counter(longList)\n",
    "    markList = re.sub(\"[^\\w]\", \" \",  markFreq).split()\n",
    "    markCounts = Counter(markList)\n",
    "    mathList = re.sub(\"[^\\w]\", \" \",  mathFreq).split()\n",
    "    mathCounts = Counter(mathList)\n",
    "    ignatList = re.sub(\"[^\\w]\", \" \",  ignatFreq).split()\n",
    "    ignatCounts = Counter(ignatList)\n",
    "    testList = re.sub(\"[^\\w]\", \" \",  testFreq).split()\n",
    "    testCounts = Counter(testList)\n",
    "\n",
    "    print(longCounts.most_common(10))\n",
    "    print(markCounts.most_common(10))\n",
    "    print(mathCounts.most_common(10))\n",
    "    print(ignatCounts.most_common(10))\n",
    "    print(testCounts.most_common(10))\n",
    "    \n",
    "    countList = []\n",
    "    L = 0\n",
    "    Ma = 0\n",
    "    Mr = 0\n",
    "    Ig = 0\n",
    "    prevWords = []\n",
    "    wordCount = 0\n",
    "    for word in testCounts:\n",
    "        label = 'notinany'\n",
    "        minDiff = 99999 \n",
    "        count = 0\n",
    "        testWordFreq = getFreq(word, testFreq)\n",
    "        if (word in longCounts):\n",
    "            tempFreq = getFreq(word, longFreq)\n",
    "            if (isSignificant(tempFreq, len(longList), testWordFreq, len(testList))):\n",
    "                L += 1\n",
    "            tempDiff = abs(longCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Longus\"\n",
    "                minDiff = tempDiff\n",
    "        if (word in markCounts):\n",
    "            tempFreq = getFreq(word, markFreq)\n",
    "            if (isSignificant(tempFreq, len(markList), testWordFreq, len(testList))):     \n",
    "                Mr += 1\n",
    "            tempDiff = abs(markCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Mark\"\n",
    "                minDiff = tempDiff\n",
    "        if (word in mathCounts):\n",
    "            tempFreq = getFreq(word, mathFreq)\n",
    "            if (isSignificant(tempFreq, len(mathList), testWordFreq, len(testList))):     \n",
    "                Ma += 1\n",
    "            tempDiff = abs(mathCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Mathew\"\n",
    "                minDiff = tempDiff\n",
    "        if (word in ignatCounts):\n",
    "            tempFreq = getFreq(word, ignatFreq)\n",
    "            if (isSignificant(tempFreq, len(ignatList), testWordFreq, len(testList))):     \n",
    "                Ig += 1\n",
    "            tempDiff = abs(ignatCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Ignatius\"\n",
    "                minDiff = tempDiff\n",
    "        if (label != 'notinany' and count > 3):\n",
    "            countList.append(label)\n",
    "            wordCount += 1\n",
    "        prevWords.append(word)\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    list = Counter(countList)\n",
    "    print(OrderedDict([(i, str(round(count / sum(list.values()) * 100.0, 3)) + '%') for i, count in list.most_common()]))\n",
    "    print(L, Ma, Mr, Ig)\n",
    "    print(L/(wordCount), Ma/(wordCount), Mr/(wordCount), Ig/(wordCount))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὰς', 152), ('ἦν', 98), ('Δάφνις', 96), ('Καὶ', 71), ('Χλόη', 64), ('Χλόην', 63), ('ταῖς', 60), ('Δάφνιν', 58), ('τοῦτο', 46), ('αὐτοῖς', 46)]\n",
      "[('καὶ', 407), ('αὐτοῦ', 173), ('αὐτῶ', 121), ('αὐτοῖς', 120), ('αὐτὸν', 117), ('τῶ', 77), ('ἵνα', 64), ('λέγει', 62), ('αὐτόν', 61), ('εἶπεν', 59)]\n",
      "[('καὶ', 276), ('αὐτοῦ', 266), ('αὐτῶ', 170), ('τῶ', 149), ('εἶπεν', 119), ('ἰησοῦς', 111), ('ὑμῖν', 107), ('αὐτοῖς', 103), ('αὐτῶν', 100), ('σου', 98)]\n",
      "[('θεοῦ', 68), ('Ἰησοῦ', 39), ('ἵνα', 36), ('Χριστοῦ', 34), ('ὑμῶν', 34), ('μοι', 34), ('αὐτοῦ', 27), ('ὑμῖν', 25), ('με', 23), ('ὑμᾶς', 22)]\n",
      "[('θεοῦ', 54), ('Ἰησοῦ', 50), ('ὑμῶν', 47), ('ὑμᾶς', 35), ('αὐτοῦ', 32), ('Χριστοῦ', 30), ('ἵνα', 28), ('ὑμῖν', 27), ('ἐστιν', 27), ('Χριστῷ', 20)]\n",
      "OrderedDict([('Ignatius', '42.657%'), ('Longus', '27.972%'), ('Mark', '23.077%'), ('Mathew', '6.294%')])\n",
      "71 62 39 10\n",
      "0.4965034965034965 0.43356643356643354 0.2727272727272727 0.06993006993006994\n"
     ]
    }
   ],
   "source": [
    "analyzeFreq() # no stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('καὶ', 1378), ('δὲ', 547), ('τὴν', 320), ('μὲν', 278), ('τὸν', 225), ('τῶν', 224), ('τὸ', 209), ('ὁ', 206), ('τὰ', 180), ('τῆς', 179)]\n",
      "[('καὶ', 1082), ('ὁ', 236), ('αὐτοῦ', 173), ('εἰς', 168), ('δὲ', 153), ('τὸν', 149), ('ἐν', 135), ('τοῦ', 132), ('τὸ', 130), ('τὴν', 126)]\n",
      "[('καὶ', 1175), ('ὁ', 493), ('δὲ', 471), ('τοῦ', 294), ('ἐν', 293), ('αὐτοῦ', 266), ('τὸ', 227), ('οἱ', 224), ('τὸν', 221), ('εἰς', 218)]\n",
      "[('καὶ', 205), ('ἐν', 100), ('θεοῦ', 68), ('τὸ', 60), ('τοῦ', 57), ('δὲ', 54), ('εἰς', 52), ('ὡς', 45), ('Ἰησοῦ', 39), ('ἵνα', 36)]\n",
      "[('καὶ', 185), ('ἐν', 135), ('τοῦ', 66), ('θεοῦ', 54), ('Ἰησοῦ', 50), ('τὸ', 50), ('εἰς', 47), ('ὑμῶν', 47), ('τῷ', 41), ('γὰρ', 38)]\n",
      "OrderedDict([('Ignatius', '51.042%'), ('Longus', '22.396%'), ('Mark', '20.312%'), ('Mathew', '6.25%')])\n",
      "91 90 66 13\n",
      "0.4739583333333333 0.46875 0.34375 0.06770833333333333\n"
     ]
    }
   ],
   "source": [
    "analyzeFreq() #stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all similar words of the texts, excluding puncuation, then returns list\n",
    "def getAllSimWords():\n",
    "    simWords = []\n",
    "    for word in testList:\n",
    "        puncuation = [\",\", \".\"]\n",
    "        if (word in longList and word in markList and word in mathList and word in ignatList and word in testList and word not in puncuation):\n",
    "            simWords.append(word)\n",
    "    return simWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὰς', 152), ('ἦν', 98), ('Δάφνις', 96), ('Καὶ', 71), ('Χλόη', 64), ('Χλόην', 63), ('ταῖς', 60), ('Δάφνιν', 58), ('τοῦτο', 46), ('αὐτοῖς', 46)]\n",
      "[('καὶ', 407), ('αὐτοῦ', 173), ('αὐτῶ', 121), ('αὐτοῖς', 120), ('αὐτὸν', 117), ('τῶ', 77), ('ἵνα', 64), ('λέγει', 62), ('αὐτόν', 61), ('εἶπεν', 59)]\n",
      "[('καὶ', 276), ('αὐτοῦ', 266), ('αὐτῶ', 170), ('τῶ', 149), ('εἶπεν', 119), ('ἰησοῦς', 111), ('ὑμῖν', 107), ('αὐτοῖς', 103), ('αὐτῶν', 100), ('σου', 98)]\n",
      "[('θεοῦ', 68), ('Ἰησοῦ', 39), ('ἵνα', 36), ('Χριστοῦ', 34), ('ὑμῶν', 34), ('μοι', 34), ('αὐτοῦ', 27), ('ὑμῖν', 25), ('με', 23), ('ὑμᾶς', 22)]\n",
      "[('θεοῦ', 54), ('Ἰησοῦ', 50), ('ὑμῶν', 47), ('ὑμᾶς', 35), ('αὐτοῦ', 32), ('Χριστοῦ', 30), ('ἵνα', 28), ('ὑμῖν', 27), ('ἐστιν', 27), ('Χριστῷ', 20)]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import re\n",
    "#creates counter of each word in the documents\n",
    "longList = re.sub(\"[^\\w]\", \" \",  longFreq).split()\n",
    "longCounts = Counter(longList)\n",
    "markList = re.sub(\"[^\\w]\", \" \",  markFreq).split()\n",
    "markCounts = Counter(markList)\n",
    "mathList = re.sub(\"[^\\w]\", \" \",  mathFreq).split()\n",
    "mathCounts = Counter(mathList)\n",
    "ignatList = re.sub(\"[^\\w]\", \" \",  ignatFreq).split()\n",
    "ignatCounts = Counter(ignatList)\n",
    "testList = re.sub(\"[^\\w]\", \" \",  testFreq).split()\n",
    "testCounts = Counter(testList)\n",
    "print(longCounts.most_common(10))\n",
    "print(markCounts.most_common(10))\n",
    "print(mathCounts.most_common(10))\n",
    "print(ignatCounts.most_common(10))\n",
    "print(testCounts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates all shared words between the documents\n",
    "commonWordList = getAllSimWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#computes cosine similarity of vector A and vector B\n",
    "def cosineSim(A, B):\n",
    "    sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "    if np.isnan(np.sum(sim)):\n",
    "        return 0\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = longCounts & markCounts & mathCounts & ignatCounts & testCounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 9 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "(('Τοιαῦτα', 'p-p---na-'), ('δὲ', 'd--------'), ('αὐτῶν', 'p-p---mg-'), ('παιζόντων', 'v-pppamg-'), ('τοιάνδε', 'a-s---fa-'), ('σπουδὴν', 'n-s---fa-'), ('Ἔρως', 'n-s---mn-'), ('ἀνέπλασε', 'v3saia---'), ('.', 'u--------'))\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/alpheios-project/arethusa-configs/blob/master/configs/arethusa.morph/gr_attributes2.json\n",
    "# The possible tags for each item of morphology\n",
    "  #  pos_tags = ('l', 'n', 'a', 'r', 'c', 'i', 'p', 'v', 'd', 'm', 'g', 'u')\n",
    "    #l - article\n",
    "    #n - noun\n",
    "    #a - adjective\n",
    "    #r - adposition\n",
    "    #c - conjugation\n",
    "    #i - interjection\n",
    "    #p - pronoun\n",
    "    #v - verb\n",
    "    #d - adverb\n",
    "    #m - numeral\n",
    "    #u - puncuation\n",
    "    #x - irregular\n",
    "    #g - participle\n",
    "    \n",
    "    \n",
    "  #  person_tags = ('1', '2', '3')\n",
    "\n",
    "  #  number_tags = ('s', 'p', 'd')\n",
    "    \n",
    "  # tense_tags = ('p', 'i', 'r', 'l', 't', 'f', 'a')       \n",
    "    #p - present\n",
    "    #i - imperfect\n",
    "    #r - perfect\n",
    "    #l - plusquamperfect\n",
    "    #t - future perfect\n",
    "    #f - future\n",
    "    #a - aorist\n",
    "    \n",
    "  #  mood_tags = ('i', 's', 'n', 'm', 'p', 'o')\n",
    "    #i - indicative\n",
    "    #s - subjunctive\n",
    "    #n - infinitive\n",
    "    #m - imperative\n",
    "    #p - participle\n",
    "    #o - optative\n",
    "    \n",
    "  #  voice_tags = ('a', 'p', 'm', 'e')\n",
    "    #a - active\n",
    "    #p - passive\n",
    "    #m - middle\n",
    "    #e - medio-passive\n",
    "\n",
    "  #  gender_tags = ('m', 'f', 'n')\n",
    "    \n",
    "  #  case_tags = ('n', 'g', 'd', 'a', 'v')\n",
    "    \n",
    "  #  degree_tags = ('p', 'c', 's')\n",
    "    #p - positive\n",
    "    #c - comparative\n",
    "    #s - superlative\n",
    "    \n",
    "#testing POS tagger\n",
    "from angel import tag\n",
    "greek_string = 'Τοιαῦτα δὲ αὐτῶν παιζόντων τοιάνδε σπουδὴν Ἔρως ἀνέπλασε.'\n",
    "\n",
    "results = tag(greek_string)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 22671 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 12741 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 20659 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 4738 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 4453 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n"
     ]
    }
   ],
   "source": [
    "#POS tagging for each document (this took forever)\n",
    "from angel import tag\n",
    "longTags = tag(long)\n",
    "markTags = tag(mark)\n",
    "mathTags = tag(math)\n",
    "ignatTags = tag(ignat)\n",
    "testTags = tag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in  list of all tagLists \"tags\", a word to look for \"testCase\", and the index of the POS tag of interest\n",
    "#returns frequencies of testCase in each document, and G-test result of the frequencies with the test frequencies\n",
    "def freqOf(tags, testCase, index):\n",
    "    results = []\n",
    "    for tagList in tags:\n",
    "        counter = 0\n",
    "        for word in tagList:\n",
    "            if (word[1][index] == testCase):\n",
    "                counter += 1\n",
    "        results.append(counter)\n",
    "    \n",
    "    print(results)\n",
    "    i = 0\n",
    "    for case in results[:-1]:\n",
    "        print(\"Testing index \", i, \" with test:\")\n",
    "        print(\"Frequencies: \", round(case/len(tags[i]), 5), \" \", round(results[4]/len(tags[4]),5))\n",
    "        if (case == 0):\n",
    "            case = 0.000000000000000000001 #if case stays 0, there is an error with the g-test\n",
    "        if (isSignificant(case, len(tags[i]), results[4], len(tags[4]))): #index 4 is index of test document\n",
    "            print(\"Different\")\n",
    "        else:\n",
    "            print(\"Not Different\")\n",
    "        print('\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 1, 0, 6, 6]\n",
      "Testing index  0  with test:\n",
      "Frequencies:  0.00283   0.00188\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  1  with test:\n",
      "Frequencies:  0.00011   0.00188\n",
      "Different\n",
      "\n",
      "\n",
      "Testing index  2  with test:\n",
      "Frequencies:  0.0   0.00188\n",
      "Different\n",
      "\n",
      "\n",
      "Testing index  3  with test:\n",
      "Frequencies:  0.00174   0.00188\n",
      "Not Different\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing frequency of optative mood usage \n",
    "tags = [longTags, markTags, mathTags, ignatTags, testTags]\n",
    "indexMood = 4\n",
    "freqOf(tags, \"o\", indexMood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 0, 0]\n",
      "Testing index  0  with test:\n",
      "Frequencies:  0.00013   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  1  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  2  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  3  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "Not Different\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing frequency of future perfect tense usage\n",
    "indexTense = 3\n",
    "freqOf(tags, \"t\", indexTense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 8, 7, 0, 0]\n",
      "Testing index  0  with test:\n",
      "Frequencies:  0.0034   0.0\n",
      "Different\n",
      "\n",
      "\n",
      "Testing index  1  with test:\n",
      "Frequencies:  0.00085   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  2  with test:\n",
      "Frequencies:  0.00047   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  3  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "Not Different\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing frequency of pluperfect tense usage\n",
    "freqOf(tags, \"l\", indexTense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a sentence, and splits the sentence\n",
    "#returns the split sentence as a tuple\n",
    "def splitSentence(sentence, i):\n",
    "    first = \"\"\n",
    "    second = \"\"\n",
    "    found = False\n",
    "    for word in sentence:\n",
    "        splitWords = [\"καί\", \"δέ\", \"ἀλλά\", \"τε\", \"ἄρα\", \"διό\"]\n",
    "        if (word in splitWords and len(first) > 2):\n",
    "            found = True\n",
    "        if (found):\n",
    "            second += \" \" + word\n",
    "        else:\n",
    "            first += \" \" + word\n",
    "    return(first, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a tagList and returns counts of participles before or after main verb\n",
    "def verbCheck(tagList):\n",
    "    leftCount = 0\n",
    "    rightCount = 0\n",
    "    mainVerbFound = False\n",
    "    for word in tagList:\n",
    "        if ((word[1][4] == 'i' or word[1][4] == 'm') and word[1][0] == 'v' and mainVerbFound == False):\n",
    "            mainVerbFound = True\n",
    "            leftCount += 1\n",
    "        else:\n",
    "            rightCount += 1\n",
    "    return (leftCount, rightCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is an algorithm to iterate a corpus, while also 'keeping up' with the tag list. \n",
    "#This is necessary because tagging sentence by sentence is VERY inefficient and takes too long,\n",
    "#so figuring out a way to iterate the corpus, while keeping on the same word as the tag list is important\n",
    "#this algorithm is used below in particpleAnalysis\n",
    "import string\n",
    "j = 0\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "splitWords = [\"καί\", \"δέ\", \"ἀλλά\", \"τε\", \"ἄρα\", \"διό\"]\n",
    "for sent in testSent:\n",
    "    sentenceSplit = sent.split()\n",
    "    for word in sentenceSplit:\n",
    "        if (testTags[j][0] in string.punctuation):\n",
    "            j += 1\n",
    "        testWord = word.translate(table)\n",
    "        line = testWord\n",
    "        testWord = ''.join( c for c in line if  c not in '’᾿·' ) #to fix bug with greek punctuation\n",
    "        testWord2 = testTags[j][0].translate(table)\n",
    "        line = testWord2\n",
    "        testWord2 = ''.join( c for c in line if  c not in '’᾿·' )\n",
    "        if (len(testWord2) < 1): #to fix a bug with multiple spaces present in word\n",
    "            j += 2\n",
    "        if (testWord == testWord2):\n",
    "            j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a text and a tag list, returns count of participles before and after main verb\n",
    "def participleAnalysis(txt, tagList):\n",
    "    import string\n",
    "    j = 0\n",
    "    leftCount = 0\n",
    "    rightCount = 0\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "    for sent in txt:\n",
    "        sentenceSplit = sent.split()\n",
    "       # print(\"sent: \", sentenceSplit)\n",
    "        firstHalf = []\n",
    "        secondHalf = []\n",
    "        enterSecond = False\n",
    "        for word in sentenceSplit:\n",
    "            if (longTags[j][0] in string.punctuation):\n",
    "                j += 1\n",
    "            testWord = word.translate(table)\n",
    "            line = testWord\n",
    "            testWord = ''.join( c for c in line if  c not in '’᾿·' ) #to fix a bug with greek punctuation\n",
    "            testWord2 = tagList[j][0].translate(table)\n",
    "            line = testWord2\n",
    "            testWord2 = ''.join( c for c in line if  c not in '’᾿·' )\n",
    "            if (len(testWord2) < 1): #to fix a bug with multiple spaces present in word\n",
    "                j += 2\n",
    "            if (testWord == testWord2):\n",
    "\n",
    "                firstHalf.append(tagList[j])\n",
    "                if (testWord in splitWords and len(firstHalf) > 3 and enterSecond == False):\n",
    "                    enterSecond = True\n",
    "                j += 1\n",
    "            if (enterSecond == True):\n",
    "                secondHalf.append(tagList[j])\n",
    "        if (enterSecond == True):\n",
    "            count = verbCheck(firstHalf)\n",
    "            leftCount += count[0]\n",
    "            rightCount += count[1]\n",
    "            count = verbCheck(secondHalf)\n",
    "            leftCount += count[0]\n",
    "            rightCount += count[1]\n",
    "        if (enterSecond == False):\n",
    "            count = verbCheck(firstHalf)\n",
    "            leftCount += count[0]\n",
    "            rightCount += count[1]\n",
    "    print(leftCount)\n",
    "    print(rightCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762\n",
      "21335\n"
     ]
    }
   ],
   "source": [
    "#Longus\n",
    "participleAnalysis(longSent, longTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "1131\n"
     ]
    }
   ],
   "source": [
    "#Mark\n",
    "participleAnalysis(markSent, markTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "2252\n"
     ]
    }
   ],
   "source": [
    "#Mathew\n",
    "participleAnalysis(mathSent, mathTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "2883\n"
     ]
    }
   ],
   "source": [
    "#Ignatius\n",
    "participleAnalysis(ignatSent, ignatTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "3020\n"
     ]
    }
   ],
   "source": [
    "participleAnalysis(testSent, testTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSignificant(762, 21335, 143, 3020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSignificant(64, 1131, 143, 3020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSignificant(103, 2252, 143, 3020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSignificant(181, 2883, 143, 3020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
