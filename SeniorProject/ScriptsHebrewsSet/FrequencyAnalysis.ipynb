{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PreprocessLib.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb #allows access to import other notebooks\n",
    "from PreprocessLib import cleanText, removeSingleLists, process_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns file in string format\n",
    "def stringify(a_file):\n",
    "    string_without_line_breaks = \"\"\n",
    "    for line in a_file:\n",
    "        stripped_line = line.rstrip() + \" \"\n",
    "        string_without_line_breaks += stripped_line\n",
    "    a_file.close()\n",
    "    sentences = string_without_line_breaks\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clemFile = open(\"./finalTexts/clement.txt\", \"r\", encoding=\"utf8\")\n",
    "lukeFile = open(\"./finalTexts/luke.txt\", \"r\", encoding=\"utf8\")\n",
    "pastFile = open(\"./finalTexts/pastoral.txt\", \"r\", encoding=\"utf8\")\n",
    "paulFile = open(\"./finalTexts/paul.txt\", \"r\", encoding=\"utf8\")\n",
    "testFile = open(\"./finalTexts/hebrews.txt\", \"r\", encoding=\"utf8\")\n",
    "clemFreq = process_file(clemFile, False)\n",
    "lukeFreq = process_file(lukeFile, False)\n",
    "pastFreq = process_file(pastFile, False)\n",
    "paulFreq = process_file(paulFile, False)\n",
    "testFreq = process_file(testFile, False)\n",
    "\n",
    "clemFile = open(\"./finalTexts/clement.txt\", \"r\", encoding=\"utf8\")\n",
    "lukeFile = open(\"./finalTexts/luke.txt\", \"r\", encoding=\"utf8\")\n",
    "pastFile = open(\"./finalTexts/pastoral.txt\", \"r\", encoding=\"utf8\")\n",
    "paulFile = open(\"./finalTexts/paul.txt\", \"r\", encoding=\"utf8\")\n",
    "testFile = open(\"./finalTexts/hebrews.txt\", \"r\", encoding=\"utf8\")\n",
    "#sentence tokenizes the documents\n",
    "clem = stringify(clemFile)\n",
    "luke = stringify(lukeFile)\n",
    "past = stringify(pastFile)\n",
    "paul = stringify(paulFile)\n",
    "test = stringify(testFile)\n",
    "from nltk import tokenize\n",
    "clemSent = tokenize.sent_tokenize(clem)\n",
    "lukeSent = tokenize.sent_tokenize(luke)\n",
    "pastSent = tokenize.sent_tokenize(past)\n",
    "paulSent = tokenize.sent_tokenize(paul)\n",
    "testSent = tokenize.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10233\n",
      "37949\n",
      "3492\n",
      "24121\n",
      "4955\n"
     ]
    }
   ],
   "source": [
    "print(len(clemFreq.split()))\n",
    "print(len(lukeFreq.split()))\n",
    "print(len(pastFreq.split()))\n",
    "print(len(paulFreq.split()))\n",
    "print(len(testFreq.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses a g-test, useful for finding differences in proportions\n",
    "#returns signifcance of G-Test\n",
    "def isSignificant(obA, exA, obB, exB):\n",
    "    import numpy as np\n",
    "    from scipy.stats import chi2_contingency\n",
    "    obs = np.array([[obA, exA-obA], [obB, exB-obB]])\n",
    "    p = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
    "    pVal = p[1] #p[1] contains the p-value of the test\n",
    "   # print(pVal)\n",
    "    if (pVal < 0.05):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "#returns list of instances and count in sorted order from most to least common\n",
    "def mostCommon(instances):\n",
    "    return sorted(sorted(Counter(instances).items(), key=itemgetter(0)), key=itemgetter(1), reverse=True)\n",
    "\n",
    "#returns frequency of word in txt\n",
    "def getFreq(word, txt):\n",
    "    return txt.count(word)\n",
    "\n",
    "#analyzes frequency of the words in the document\n",
    "#returns most common words of each document and count of significantly different word frequencies\n",
    "# and each iteration counts which authors words was most similar, then returns the list\n",
    "def analyzeFreq():\n",
    "    from scipy import stats\n",
    "    from collections import Counter\n",
    "    clemList = re.sub(\"[^\\w]\", \" \",  clemFreq).split()\n",
    "    clemCounts = Counter(clemList)\n",
    "    lukeList = re.sub(\"[^\\w]\", \" \",  lukeFreq).split()\n",
    "    lukeCounts = Counter(lukeList)\n",
    "    pastList = re.sub(\"[^\\w]\", \" \",  pastFreq).split()\n",
    "    pastCounts = Counter(pastList)\n",
    "    paulList = re.sub(\"[^\\w]\", \" \",  paulFreq).split()\n",
    "    paulCounts = Counter(paulList)\n",
    "    testList = re.sub(\"[^\\w]\", \" \",  testFreq).split()\n",
    "    testCounts = Counter(testList)\n",
    "\n",
    "    print(clemCounts.most_common(10))\n",
    "    print(lukeCounts.most_common(10))\n",
    "    print(pastCounts.most_common(10))\n",
    "    print(paulCounts.most_common(10))\n",
    "    print(testCounts.most_common(10))\n",
    "    \n",
    "    countList = []\n",
    "    Cl = 0\n",
    "    Lu = 0\n",
    "    Pa = 0\n",
    "    Pu = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "    for word in testCounts:\n",
    "        label = 'notinany'\n",
    "        minDiff = 99999 \n",
    "        count = 0\n",
    "        testWordFreq = getFreq(word, testFreq)\n",
    "\n",
    "        if (word in clemCounts):\n",
    "            tempFreq = getFreq(word, clemFreq)\n",
    "            if (isSignificant(tempFreq, len(clemList), testWordFreq, len(testList))):\n",
    "                Cl += 1\n",
    "            tempDiff = abs(clemCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Clement\"\n",
    "                minDiff = tempDiff\n",
    "        if (word in lukeCounts):\n",
    "            tempFreq = getFreq(word, lukeFreq)\n",
    "            if (isSignificant(tempFreq, len(lukeList), testWordFreq, len(testList))):     \n",
    "                Lu += 1\n",
    "            tempDiff = abs(lukeCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Luke\"\n",
    "                minDiff = tempDiff\n",
    "        if (word in pastCounts):\n",
    "            tempFreq = getFreq(word, pastFreq)\n",
    "            if (isSignificant(tempFreq, len(pastList), testWordFreq, len(testList))):     \n",
    "                Pa += 1\n",
    "            tempDiff = abs(pastCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Pastoral\"\n",
    "                minDiff = tempDiff\n",
    "        if (word in paulCounts):\n",
    "            tempFreq = getFreq(word, paulFreq)\n",
    "            if (isSignificant(tempFreq, len(paulList), testWordFreq, len(testList))):     \n",
    "                Pu += 1\n",
    "            tempDiff = abs(paulCounts[word] - testCounts[word])\n",
    "            count += 1\n",
    "            if (tempDiff < minDiff):\n",
    "                label = \"Paul\"\n",
    "                minDiff = tempDiff\n",
    "        if (label != 'notinany' and count > 3):\n",
    "            countList.append(label)\n",
    "            wordCount += 1\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "    list = Counter(countList)\n",
    "    print(OrderedDict([(i, str(round(count / sum(list.values()) * 100.0, 3)) + '%') for i, count in list.most_common()]))\n",
    "    print(Cl, Lu, Pa, Pu)\n",
    "    print(Cl/wordCount, Lu/wordCount, Pa/wordCount, Pu/wordCount)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('καὶ', 677), ('ἐν', 186), ('τοῦ', 169), ('αὐτοῦ', 165), ('τῆς', 143), ('τὸ', 134), ('εἰς', 116), ('ὁ', 115), ('τῶν', 113), ('τὴν', 95)]\n",
      "[('καὶ', 2560), ('δὲ', 1034), ('τοῦ', 707), ('ὁ', 707), ('ἐν', 635), ('εἰς', 527), ('τὸν', 489), ('τὸ', 422), ('τὴν', 403), ('αὐτοῦ', 384)]\n",
      "[('καὶ', 196), ('ἐν', 94), ('δὲ', 58), ('τοῦ', 46), ('τὴν', 42), ('εἰς', 39), ('μὴ', 39), ('τῆς', 37), ('ὁ', 37), ('θεοῦ', 35)]\n",
      "[('καὶ', 1051), ('ἐν', 674), ('δὲ', 493), ('τοῦ', 454), ('τὸ', 377), ('ὁ', 361), ('γὰρ', 356), ('εἰς', 316), ('τῶ', 263), ('μὴ', 251)]\n",
      "[('καὶ', 255), ('γὰρ', 87), ('τῆς', 85), ('εἰς', 74), ('τὸν', 74), ('ὁ', 69), ('τὴν', 69), ('τὸ', 68), ('δὲ', 67), ('τοῦ', 67)]\n",
      "OrderedDict([('Pastoral', '50.427%'), ('Clement', '40.171%'), ('Luke', '6.838%'), ('Paul', '2.564%')])\n",
      "57 124 37 113\n",
      "0.24358974358974358 0.5299145299145299 0.1581196581196581 0.4829059829059829\n"
     ]
    }
   ],
   "source": [
    "analyzeFreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 12563 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 42423 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 4071 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 27409 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 5590 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n"
     ]
    }
   ],
   "source": [
    "#POS tags the documents\n",
    "from angel import tag\n",
    "clemTags = tag(clem)\n",
    "lukeTags = tag(luke)\n",
    "pastTags = tag(past)\n",
    "paulTags = tag(paul)\n",
    "testTags = tag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in  list of all tagLists \"tags\", a word to look for \"testCase\", and the index of the POS tag of interest\n",
    "#returns frequencies of testCase in each document, and G-test result of the frequencies with the test frequencies\n",
    "def freqOf(tags, testCase, index):\n",
    "    results = []\n",
    "    for tagList in tags:\n",
    "        counter = 0\n",
    "        for word in tagList:\n",
    "            if (word[1][index] == testCase):\n",
    "                counter += 1\n",
    "        results.append(counter)\n",
    "    \n",
    "    print(results)\n",
    "    i = 0\n",
    "    for case in results[:-1]:\n",
    "        print(\"Testing index \", i, \" with test:\")\n",
    "        print(\"Frequencies: \", round(case/len(tags[i]), 5), \" \", round(results[4]/len(tags[4]),5))\n",
    "        if (case == 0):\n",
    "            case = 0.000000000000000000001 #if case stays 0, there is an error with the g-test\n",
    "        if (isSignificant(case, len(tags[i]), results[4], len(tags[4]))):\n",
    "            print(\"Different\")\n",
    "        else:\n",
    "            print(\"Not Different\")\n",
    "        print('\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 29, 2, 23, 0]\n",
      "Testing index  0  with test:\n",
      "Frequencies:  0.00064   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  1  with test:\n",
      "Frequencies:  0.00068   0.0\n",
      "Different\n",
      "\n",
      "\n",
      "Testing index  2  with test:\n",
      "Frequencies:  0.00049   0.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  3  with test:\n",
      "Frequencies:  0.00084   0.0\n",
      "Different\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing frequency of optative mood usage \n",
    "tags = [clemTags, lukeTags, pastTags, paulTags, testTags]\n",
    "indexMood = 4\n",
    "freqOf(tags, \"o\", indexMood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "Testing index  0  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "1.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  1  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "1.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  2  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "1.0\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  3  with test:\n",
      "Frequencies:  0.0   0.0\n",
      "1.0\n",
      "Not Different\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing frequency of future perfect tense usage\n",
    "indexTense = 3\n",
    "freqOf(tags, \"t\", indexTense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 39, 3, 7, 4]\n",
      "Testing index  0  with test:\n",
      "Frequencies:  0.00048   0.00072\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  1  with test:\n",
      "Frequencies:  0.00092   0.00072\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  2  with test:\n",
      "Frequencies:  0.00074   0.00072\n",
      "Not Different\n",
      "\n",
      "\n",
      "Testing index  3  with test:\n",
      "Frequencies:  0.00026   0.00072\n",
      "Not Different\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing frequency of pluperfect tense usage\n",
    "freqOf(tags, \"l\", indexTense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a tagList and returns counts of participles before or after main verb\n",
    "def verbCheck(tagList):\n",
    "    leftCount = 0\n",
    "    rightCount = 0\n",
    "    mainVerbFound = False\n",
    "    for word in tagList:\n",
    "        if ((word[1][4] == 'i' or word[1][4] == 'm') and word[1][0] == 'v' and mainVerbFound == False):\n",
    "            mainVerbFound = True\n",
    "            leftCount += 1\n",
    "        else:\n",
    "            rightCount += 1\n",
    "    return (leftCount, rightCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitWords = [\"καί\", \"δέ\", \"ἀλλά\", \"τε\", \"ἄρα\", \"διό\"]\n",
    "#takes in a text and a tag list, returns count of participles before and after main verb\n",
    "def participleAnalysis(txt, tagList):\n",
    "    import string\n",
    "    j = 0\n",
    "    leftCount = 0\n",
    "    rightCount = 0\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "    for sent in txt:\n",
    "        sentenceSplit = sent.split()\n",
    "       # print(\"sent: \", sentenceSplit)\n",
    "        firstHalf = []\n",
    "        secondHalf = []\n",
    "        enterSecond = False\n",
    "        for word in sentenceSplit:\n",
    "            if (tagList[j][0] in string.punctuation):\n",
    "                j += 1\n",
    "            testWord = word.translate(table)\n",
    "            line = testWord\n",
    "            testWord = ''.join( c for c in line if  c not in '’᾿·0123456789' ) #to fix a bug with greek punctuation\n",
    "            testWord2 = tagList[j][0].translate(table)\n",
    "            line = testWord2\n",
    "            testWord2 = ''.join( c for c in line if  c not in '’᾿·0123456789' )\n",
    "            if (len(testWord2) < 1): #to fix a bug with multiple spaces present in word\n",
    "                j += 2\n",
    "            if (testWord == testWord2):\n",
    "                firstHalf.append(tagList[j])\n",
    "                if (testWord in splitWords and len(firstHalf) > 3 and enterSecond == False):\n",
    "                    enterSecond = True\n",
    "                j += 1\n",
    "            if (enterSecond == True):\n",
    "                secondHalf.append(tagList[j])\n",
    "        if (enterSecond == True):\n",
    "            count = verbCheck(firstHalf)\n",
    "            leftCount += count[0]\n",
    "            rightCount += count[1]\n",
    "            count = verbCheck(secondHalf)\n",
    "            leftCount += count[0]\n",
    "            rightCount += count[1]\n",
    "        if (enterSecond == False):\n",
    "            count = verbCheck(firstHalf)\n",
    "            leftCount += count[0]\n",
    "            rightCount += count[1]\n",
    "    print(leftCount)\n",
    "    print(rightCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n",
      "10347\n",
      "0.0979999974185213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participleAnalysis(clemSent, clemTags)\n",
    "isSignificant(366,10347,161,5324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635\n",
      "38760\n",
      "2.0513469283665826e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participleAnalysis(lukeSent, lukeTags)\n",
    "isSignificant(1635,38760,161,5324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "3397\n",
      "0.04226510799143014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participleAnalysis(pastSent, pastTags)\n",
    "isSignificant(131,3397,161,5324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898\n",
      "23984\n",
      "0.010443997278349841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participleAnalysis(paulSent, paulTags)\n",
    "isSignificant(898,23984,161,5324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "5324\n"
     ]
    }
   ],
   "source": [
    "participleAnalysis(testSent, testTags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
