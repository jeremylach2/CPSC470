{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PreprocessLib.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb #allows access to import other notebooks\n",
    "from PreprocessLib import cleanText, removeSingleLists, process_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processes files for Word2Vec\n",
    "clemFile = open(\"./finalTexts/clement.txt\", \"r\", encoding=\"utf8\")\n",
    "lukeFile = open(\"./finalTexts/luke.txt\", \"r\", encoding=\"utf8\")\n",
    "pastFile = open(\"./finalTexts/pastoral.txt\", \"r\", encoding=\"utf8\")\n",
    "paulFile = open(\"./finalTexts/paul.txt\", \"r\", encoding=\"utf8\")\n",
    "testFile = open(\"./finalTexts/hebrews.txt\", \"r\", encoding=\"utf8\")\n",
    "clem = process_file(clemFile)\n",
    "luke = process_file(lukeFile)\n",
    "past = process_file(pastFile)\n",
    "paul = process_file(paulFile)\n",
    "test = process_file(testFile)\n",
    "#processes files for POS tagging\n",
    "clemFile = open(\"./finalTexts/clement.txt\", \"r\", encoding=\"utf8\")\n",
    "lukeFile = open(\"./finalTexts/luke.txt\", \"r\", encoding=\"utf8\")\n",
    "pastFile = open(\"./finalTexts/pastoral.txt\", \"r\", encoding=\"utf8\")\n",
    "paulFile = open(\"./finalTexts/paul.txt\", \"r\", encoding=\"utf8\")\n",
    "testFile = open(\"./finalTexts/hebrews.txt\", \"r\", encoding=\"utf8\")\n",
    "clemFreq = process_file(clemFile, False)\n",
    "lukeFreq = process_file(lukeFile, False)\n",
    "pastFreq = process_file(pastFile, False)\n",
    "paulFreq = process_file(paulFile, False)\n",
    "testFreq = process_file(testFile, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-ab074bc1e01f>:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  clemModel.train(clem, total_examples=clemModel.corpus_count, epochs=clemModel.iter)\n",
      "<ipython-input-27-ab074bc1e01f>:10: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  lukeModel.train(luke, total_examples=lukeModel.corpus_count, epochs=lukeModel.iter)\n",
      "<ipython-input-27-ab074bc1e01f>:14: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  pastModel.train(past, total_examples=pastModel.corpus_count, epochs=pastModel.iter)\n",
      "<ipython-input-27-ab074bc1e01f>:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  paulModel.train(paul, total_examples=paulModel.corpus_count, epochs=paulModel.iter)\n",
      "<ipython-input-27-ab074bc1e01f>:22: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  testModel.train(test, total_examples=testModel.corpus_count, epochs=testModel.iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11183, 20000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec  \n",
    "#best model\n",
    "clemModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=100, sg=1) \n",
    "clemModel.build_vocab(clem)\n",
    "clemModel.train(clem, total_examples=clemModel.corpus_count, epochs=clemModel.iter)\n",
    "\n",
    "lukeModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=100, sg=1) \n",
    "lukeModel.build_vocab(luke)\n",
    "lukeModel.train(luke, total_examples=lukeModel.corpus_count, epochs=lukeModel.iter)\n",
    "\n",
    "pastModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=100, sg=1) \n",
    "pastModel.build_vocab(past)\n",
    "pastModel.train(past, total_examples=pastModel.corpus_count, epochs=pastModel.iter)\n",
    "\n",
    "paulModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=100, sg=1) \n",
    "paulModel.build_vocab(paul)\n",
    "paulModel.train(paul, total_examples=paulModel.corpus_count, epochs=paulModel.iter)\n",
    "\n",
    "testModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=100, sg=1) \n",
    "testModel.build_vocab(test)\n",
    "testModel.train(test, total_examples=testModel.corpus_count, epochs=testModel.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a W2V model and returns the associated vocabulary of the model\n",
    "def intoDict (model): \n",
    "    # Get the ordered list of words in the vocabulary\n",
    "    words = model.wv.vocab.keys()\n",
    "    # Make a dictionary\n",
    "    we_dict = {word:model.wv[word] for word in words}\n",
    "    return we_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a dictionary \"dct\", a list of words \"wordList\", and optionally a label\n",
    "#takes each word vector of the words in wordList and adds them to an overall vector, then returns this vector\n",
    "def createVec(dct, wordList, label=\"NA\"):\n",
    "    vec = []\n",
    "    for word in wordList:\n",
    "        vec.append(dct[word])\n",
    "    if (label != \"NA\"):\n",
    "        vec.append(label)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating feature vectors of the models\n",
    "clemDict = intoDict(clemModel)\n",
    "lukeDict = intoDict(lukeModel)\n",
    "pastDict = intoDict(pastModel)\n",
    "paulDict = intoDict(paulModel)\n",
    "testDict = intoDict(testModel)\n",
    "\n",
    "commonWordList = ['θεοῦ', 'ἡμῶν', 'τὰς', 'ἵνα', 'αὐτοῦ', 'σου', 'κύριος', 'τοῦτο']\n",
    "\n",
    "clemVec = createVec(clemDict, commonWordList, 'Clement')\n",
    "lukeVec = createVec(lukeDict, commonWordList, 'Luke')\n",
    "pastVec = createVec(pastDict, commonWordList, 'Pastoral')\n",
    "paulVec = createVec(paulDict, commonWordList, 'Paul')\n",
    "testVec = createVec(testDict, commonWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#converting to training and testing sets\n",
    "trainX = np.array((clemVec[:5], lukeVec[:5], pastVec[:5], paulVec[:5]))\n",
    "trainY =[\"Clement\", \"Luke\", \"Pastoral\", \"Paul\"]\n",
    "testX = np.array(testVec[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the vectors\n",
    "nsamples, nx, ny = trainX.shape\n",
    "trainXReshape = trainX.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=4000, hidden_layer_sizes=(4,)).fit(trainXReshape, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clement' 'Luke' 'Pastoral' 'Paul']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.07261948, 0.01284027, 0.89705575, 0.01748449]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results of MLP classif\n",
    "testXReshape = testX.reshape(1,testX.shape[1]*5)\n",
    "print(clf.classes_)\n",
    "clf.predict_proba(testXReshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clement' 'Luke' 'Pastoral' 'Paul']\n",
      "[[0.07261948 0.01284027 0.89705575 0.01748449]]\n"
     ]
    }
   ],
   "source": [
    "#saving the models\n",
    "clemModel.save(\"clemModel.model\")\n",
    "lukeModel.save(\"lukeModel.model\")\n",
    "pastModel.save(\"pastModel.model\")\n",
    "paulModel.save(\"paulModel.model\")\n",
    "testModel.save(\"testModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clement' 'Luke' 'Pastoral' 'Paul']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03176393, 0.01781514, 0.9333607 , 0.01706028]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min_count = 2, alpha=0.035, sample=0.0001, size=5, sg=1\n",
    "testXReshape = testX.reshape(1,testX.shape[1]*5)\n",
    "print(clf.classes_)\n",
    "clf.predict_proba(testXReshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clement' 'Luke' 'Pastoral' 'Paul']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12112716, 0.00412895, 0.8714757 , 0.00326816]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min_count = 2, alpha=0.025, sample=0.0001, size=100, sg=1\n",
    "testXReshape = testX.reshape(1,testX.shape[1]*5)\n",
    "print(clf.classes_)\n",
    "clf.predict_proba(testXReshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#returns cosine similarity of vector A and vector B\n",
    "def cosineSim(A, B):\n",
    "    sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "    if np.isnan(np.sum(sim)):\n",
    "        return 0\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3110, 20000)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec  \n",
    "clemModel = Word2Vec(min_count = 1, alpha=0.035, sample=0.0001, size=100, sg=1) \n",
    "clemModel.build_vocab(clem)\n",
    "clemModel.train(clem, total_examples=clemModel.corpus_count, epochs=clemModel.iter)\n",
    "\n",
    "lukeModel = Word2Vec(min_count = 1, alpha=0.035, sample=0.0001, size=100, sg=1) \n",
    "lukeModel.build_vocab(luke)\n",
    "lukeModel.train(luke, total_examples=lukeModel.corpus_count, epochs=lukeModel.iter)\n",
    "\n",
    "pastModel = Word2Vec(min_count = 1, alpha=0.035, sample=0.0001, size=100, sg=1) \n",
    "pastModel.build_vocab(past)\n",
    "pastModel.train(past, total_examples=pastModel.corpus_count, epochs=pastModel.iter)\n",
    "\n",
    "paulModel = Word2Vec(min_count = 1, alpha=0.035, sample=0.0001, size=100, sg=1) \n",
    "paulModel.build_vocab(paul)\n",
    "paulModel.train(paul, total_examples=paulModel.corpus_count, epochs=paulModel.iter)\n",
    "\n",
    "testModel = Word2Vec(min_count = 1, alpha=0.035, sample=0.0001, size=100, sg=1) \n",
    "testModel.build_vocab(test)\n",
    "testModel.train(test, total_examples=testModel.corpus_count, epochs=testModel.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in countNum, which is the minimum threshold for how many documents the word is found in\n",
    "#this function tests two things\n",
    "#1) the mean average cosine similarity of the similar words to the test document\n",
    "#2) a count of which author's words are most similar to the test document's words\n",
    "def runAnalysis(countNum = 1):\n",
    "        tDistList = []\n",
    "        ClDistList = []\n",
    "        LuDistList = []\n",
    "        PaDistList = []\n",
    "        PuDistList = []\n",
    "\n",
    "\n",
    "        countList = []\n",
    "\n",
    "        for x in testModel.wv.vocab.keys():\n",
    "            label = \"notinany\"\n",
    "            maxNum = -9999\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            if (x in testModel.wv.vocab):\n",
    "                tDist = cosineSim(testModel[x], testModel[x])\n",
    "                tDistList.append(tDist)\n",
    "                \n",
    "            if (x in clemModel.wv.vocab):\n",
    "                ClDist = cosineSim(testModel[x], clemModel[x])\n",
    "                ClDistList.append(ClDist)\n",
    "                count += 1\n",
    "                if (ClDist > maxNum):\n",
    "                    maxNum = ClDist\n",
    "                    label = \"Clement\"\n",
    "\n",
    "            if (x in lukeModel.wv.vocab):\n",
    "                LuDist = cosineSim(testModel[x], lukeModel[x]) \n",
    "                LuDistList.append(LuDist)\n",
    "                count += 1\n",
    "                if (LuDist > maxNum):\n",
    "                    maxNum = LuDist\n",
    "                    label = \"Luke\"\n",
    "\n",
    "            if (x in pastModel.wv.vocab):\n",
    "                PaDist = cosineSim(testModel[x], pastModel[x])\n",
    "                PaDistList.append(PaDist)\n",
    "                count += 1\n",
    "                if (PaDist > maxNum):\n",
    "                    maxNum = PaDist\n",
    "                    label = \"Pastoral\"\n",
    "\n",
    "            if (x in paulModel.wv.vocab):   \n",
    "                PuDist = cosineSim(testModel[x], paulModel[x])\n",
    "                PuDistList.append(PuDist)\n",
    "                count += 1\n",
    "                if (PuDist > maxNum):\n",
    "                    maxNum = PuDist\n",
    "                    label = \"Paul\"\n",
    "\n",
    "\n",
    "            if (label != 'notinany' and count > countNum):\n",
    "                countList.append(label)\n",
    "\n",
    "        \n",
    "        \n",
    "        import statistics\n",
    "        #mean of all cosine similarities of the word to the test word in the list\n",
    "        print(\"Test:\", statistics.mean(tDistList), \"Clement:\", statistics.mean(ClDistList), \"Luke:\", statistics.mean(LuDistList), \n",
    "              \"Pastoral:\", statistics.mean(PaDistList), \"Paul:\", statistics.mean(PuDistList))\n",
    "        from collections import Counter\n",
    "        from collections import OrderedDict\n",
    "        list = Counter(countList)\n",
    "        #percentage of words closest to the test word in each list\n",
    "        print(OrderedDict([(i, str(round(count / sum(list.values()) * 100.0, 3)) + '%') for i, count in list.most_common()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 1.0 Clement: 0.7853447 Luke: 0.105903685 Pastoral: 0.9840193 Paul: 0.11034792\n",
      "OrderedDict([('Pastoral', '100.0%')])\n",
      "Test: 1.0 Clement: 0.7853447 Luke: 0.105903685 Pastoral: 0.9840193 Paul: 0.11034792\n",
      "OrderedDict([('Pastoral', '62.745%'), ('Clement', '37.255%')])\n",
      "Test: 1.0 Clement: 0.7853447 Luke: 0.105903685 Pastoral: 0.9840193 Paul: 0.11034792\n",
      "OrderedDict([('Pastoral', '45.515%'), ('Clement', '33.887%'), ('Paul', '10.299%'), ('Luke', '10.299%')])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#min_count = 2, alpha=0.035, sample=0.0001, size=200, sg=1\n",
    "runAnalysis(3)\n",
    "runAnalysis(2)\n",
    "runAnalysis(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 1.0 Clement: 0.99468833 Luke: 0.099074885 Pastoral: 0.9991536 Paul: 0.21117012\n",
      "OrderedDict([('Pastoral', '100.0%')])\n",
      "Test: 1.0 Clement: 0.99468833 Luke: 0.099074885 Pastoral: 0.9991536 Paul: 0.21117012\n",
      "OrderedDict([('Pastoral', '63.889%'), ('Clement', '36.111%')])\n",
      "Test: 1.0 Clement: 0.99468833 Luke: 0.099074885 Pastoral: 0.9991536 Paul: 0.21117012\n",
      "OrderedDict([('Pastoral', '43.81%'), ('Clement', '28.571%'), ('Paul', '20.0%'), ('Luke', '7.619%')])\n"
     ]
    }
   ],
   "source": [
    "#min_count = 4, alpha=0.025, sample=0.0001, size=30, sg=1\n",
    "runAnalysis(3)\n",
    "runAnalysis(2)\n",
    "runAnalysis(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 8439 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 30454 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 19345 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 3107 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n",
      "Loading models...\n",
      "Part-of-speech models loading...\n",
      "Person models loading...\n",
      "Number models loading...\n",
      "Tense models loading...\n",
      "Mood models loading...\n",
      "Voice models loading...\n",
      "Gender models loading...\n",
      "Case models loading...\n",
      "Degree models loading...\n",
      "Pre-processing text...\n",
      "Text and punctuation split into 4017 individual tokens.\n",
      "Angel's looking at each word by itself...\n",
      "Reconsidering tags...\n",
      "Studying each word in light of its context...\n"
     ]
    }
   ],
   "source": [
    "#POS tags\n",
    "from angel import tag\n",
    "clemTags = tag(clemFreq)\n",
    "lukeTags = tag(lukeFreq)\n",
    "paulTags = tag(paulFreq)\n",
    "pastTags = tag(pastFreq)\n",
    "testTags = tag(testFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a tag list and transforms it to 3D vector format\n",
    "def prepareForW2V(tagList):\n",
    "    cleaned = [[]]\n",
    "    line = []\n",
    "    for word in tagList:\n",
    "        if (word[0] == \".\"):\n",
    "            cleaned.append(line)\n",
    "            line = []\n",
    "        else:\n",
    "            line.append(word[1])\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms the texts into vector format\n",
    "clemPrep = prepareForW2V(clemTags)\n",
    "lukePrep = prepareForW2V(lukeTags)\n",
    "paulPrep = prepareForW2V(paulTags)\n",
    "pastPrep = prepareForW2V(pastTags)\n",
    "testPrep = prepareForW2V(testTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-5d8c330931b2>:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  clemModel.train(clemPrep, total_examples=clemModel.corpus_count, epochs=clemModel.iter)\n",
      "<ipython-input-8-5d8c330931b2>:10: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  lukeModel.train(lukePrep, total_examples=lukeModel.corpus_count, epochs=lukeModel.iter)\n",
      "<ipython-input-8-5d8c330931b2>:14: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  pastModel.train(pastPrep, total_examples=pastModel.corpus_count, epochs=pastModel.iter)\n",
      "<ipython-input-8-5d8c330931b2>:18: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  paulModel.train(paulPrep, total_examples=paulModel.corpus_count, epochs=paulModel.iter)\n",
      "<ipython-input-8-5d8c330931b2>:22: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  testModel.train(testPrep, total_examples=testModel.corpus_count, epochs=testModel.iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3194, 19290)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec  \n",
    "\n",
    "clemModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=5, sg=1) \n",
    "clemModel.build_vocab(clemPrep)\n",
    "clemModel.train(clemPrep, total_examples=clemModel.corpus_count, epochs=clemModel.iter)\n",
    "\n",
    "lukeModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=5, sg=1) \n",
    "lukeModel.build_vocab(lukePrep)\n",
    "lukeModel.train(lukePrep, total_examples=lukeModel.corpus_count, epochs=lukeModel.iter)\n",
    "\n",
    "pastModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=5, sg=1) \n",
    "pastModel.build_vocab(pastPrep)\n",
    "pastModel.train(pastPrep, total_examples=pastModel.corpus_count, epochs=pastModel.iter)\n",
    "\n",
    "paulModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=5, sg=1) \n",
    "paulModel.build_vocab(paulPrep)\n",
    "paulModel.train(paulPrep, total_examples=paulModel.corpus_count, epochs=paulModel.iter)\n",
    "\n",
    "testModel = Word2Vec(min_count = 1, alpha=0.025, sample=0.0001, size=5, sg=1) \n",
    "testModel.build_vocab(testPrep)\n",
    "testModel.train(testPrep, total_examples=testModel.corpus_count, epochs=testModel.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns tags found in all the texts\n",
    "def getSimTags():\n",
    "    simWords = []\n",
    "    for word in testPrep:\n",
    "        if (word in longPrep and word in markPrep and word in mathPrep and word in ignatPrep):\n",
    "            simWords.append(word)\n",
    "    return simWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms lst to list format\n",
    "def convertToList(lst):\n",
    "    ls = []\n",
    "    for sentence in lst:\n",
    "        for word in sentence:\n",
    "            ls.append(word)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a W2V model and returns the associated vocabulary of the model\n",
    "def intoDict (model): \n",
    "    # Get the ordered list of words in the vocabulary\n",
    "    words = model.wv.vocab.keys()\n",
    "    # Make a dictionary\n",
    "    we_dict = {word:model.wv[word] for word in words}\n",
    "    return we_dict\n",
    "#takes a dictionary \"dct\", a list of words \"wordList\", and optionally a label\n",
    "#takes each word vector of the words in wordList and adds them to an overall vector, then returns this vector\n",
    "\n",
    "def createVec(dct, wordList, label=\"NA\"):\n",
    "    vec = []\n",
    "    for word in wordList:\n",
    "        vec.append(dct[word])\n",
    "    if (label != \"NA\"):\n",
    "        vec.append(label)\n",
    "    return vec\n",
    "\n",
    "#returns all similar words of the texts\n",
    "def getAllSimWords():\n",
    "    simWords = []\n",
    "    for word in testDict:\n",
    "        puncuation = [\",\", \".\"]\n",
    "        if (word in clemDict and word in lukeDict and word in pastDict and word in paulDict and word not in puncuation):\n",
    "            simWords.append(word)\n",
    "    return simWords\n",
    "\n",
    "#creating feature vectors of the models\n",
    "clemDict = intoDict(clemModel)\n",
    "lukeDict = intoDict(lukeModel)\n",
    "pastDict = intoDict(pastModel)\n",
    "paulDict = intoDict(paulModel)\n",
    "testDict = intoDict(testModel)\n",
    "\n",
    "commonWordList = getAllSimWords()\n",
    "#creating vectors to represent the models based on the commonWordList\n",
    "clemVec = createVec(clemDict, commonWordList, 'Clement')\n",
    "lukeVec = createVec(lukeDict, commonWordList, 'Luke')\n",
    "pastVec = createVec(pastDict, commonWordList, 'Pastoral')\n",
    "paulVec = createVec(paulDict, commonWordList, 'Paul')\n",
    "testVec = createVec(testDict, commonWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#creating training and testing lists\n",
    "trainX = np.array((clemVec[:len(commonWordList)], lukeVec[:len(commonWordList)], pastVec[:len(commonWordList)], paulVec[:len(commonWordList)]))\n",
    "trainY =['Clement', 'Luke', \"Pastoral\", \"Paul\"]\n",
    "testX = np.array(testVec[:len(commonWordList)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns average results of MLPClassifier n (iterations) times\n",
    "#although the variation for W2V was very small, I still wanted\n",
    "# to make sure the results were as accurate as possible\n",
    "def runIteration(iterations):\n",
    "    avCl = 0\n",
    "    avLu = 0\n",
    "    avPa = 0\n",
    "    avPu = 0\n",
    "    for i in range(0, iterations):\n",
    "        nsamples, nx, ny = trainX.shape\n",
    "        trainXReshape = trainX.reshape((nsamples,nx*ny))\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        clf = MLPClassifier(max_iter=8000, hidden_layer_sizes=(4,)).fit(trainXReshape, trainY)\n",
    "        testXReshape = testX.reshape(1,testX.shape[1]*len(commonWordList))\n",
    "        avCl += clf.predict_proba(testXReshape)[0][0]\n",
    "        avLu += clf.predict_proba(testXReshape)[0][1]\n",
    "        avPa += clf.predict_proba(testXReshape)[0][2]\n",
    "        avPu += clf.predict_proba(testXReshape)[0][3]\n",
    "    print(\"Average Clement: \", avCl/iterations)\n",
    "    print(\"Average Luke: \", avLu/iterations)\n",
    "    print(\"Average Pastoral: \", avPa/iterations)\n",
    "    print(\"Average Paul: \", avPu/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Clement:  0.1210379484184341\n",
      "Average Luke:  0.05472108666814164\n",
      "Average Pastoral:  0.768235138207674\n",
      "Average Paul:  0.05600582516389663\n"
     ]
    }
   ],
   "source": [
    "runIteration(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
