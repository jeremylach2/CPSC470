{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PreprocessLib.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb #allows access to import other notebooks\n",
    "from PreprocessLib import process_file_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "#loads models\n",
    "clemModel = Word2Vec.load(\"clemModel.model\")\n",
    "lukeModel = Word2Vec.load(\"lukeModel.model\")\n",
    "pastModel = Word2Vec.load(\"pastModel.model\")\n",
    "paulModel = Word2Vec.load(\"paulModel.model\")\n",
    "testModel = Word2Vec.load(\"testModel.model\")\n",
    "doc2VecModel = Doc2Vec.load(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a W2V model and returns the associated vocabulary of the model\n",
    "def intoDict (model): \n",
    "    # Get the ordered list of words in the vocabulary\n",
    "    words = model.wv.vocab.keys()\n",
    "    # Make a dictionary\n",
    "    we_dict = {word:model.wv[word] for word in words}\n",
    "    return we_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a dictionary \"dct\", a list of words \"wordList\", and optionally a label\n",
    "#takes each word vector of the words in wordList and adds them to an overall vector, then returns this vector\n",
    "def createVec(dct, wordList, label=\"NA\"):\n",
    "    vec = []\n",
    "    for word in wordList:\n",
    "        vec.append(dct[word])\n",
    "    if (label != \"NA\"):\n",
    "        vec.append(label)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating feature vectors of the models\n",
    "clemDict = intoDict(clemModel)\n",
    "lukeDict = intoDict(lukeModel)\n",
    "pastDict = intoDict(pastModel)\n",
    "paulDict = intoDict(paulModel)\n",
    "testDict = intoDict(testModel)\n",
    "\n",
    "commonWordList = ['θεοῦ', 'ἡμῶν', 'τὰς', 'ἵνα', 'αὐτοῦ', 'σου', 'κύριος', 'τοῦτο']\n",
    "\n",
    "clemVec = createVec(clemDict, commonWordList, 'Clement')\n",
    "lukeVec = createVec(lukeDict, commonWordList, 'Luke')\n",
    "pastVec = createVec(pastDict, commonWordList, 'Pastoral')\n",
    "paulVec = createVec(paulDict, commonWordList, 'Paul')\n",
    "testVec = createVec(testDict, commonWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "testFile = open(\"./finalTexts/Hebrews.txt\", \"r\", encoding=\"utf8\")\n",
    "#prepares D2V for concatenation \n",
    "test = process_file_doc(testFile)\n",
    "clemVec.append(doc2VecModel[0])\n",
    "lukeVec.append(doc2VecModel[1])\n",
    "pastVec.append(doc2VecModel[2])\n",
    "paulVec.append(doc2VecModel[3])\n",
    "docVecs = {}\n",
    "docVecs['Test'] = doc2VecModel.infer_vector(test.split())\n",
    "testXDoc = np.array(docVecs[\"Test\"])\n",
    "testVec.append(testXDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits into training and testing sets\n",
    "trainX = np.array((clemVec[:6], lukeVec[:6], pastVec[:6], paulVec[:6]))\n",
    "trainY =[\"Clement\", \"Luke\", \"Pastoral\", \"Paul\"]\n",
    "testX = np.array(testVec[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshapes data\n",
    "nsamples, nx, ny = trainX.shape\n",
    "trainXReshape = trainX.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=1, max_iter=4000, hidden_layer_sizes=(4,)).fit(trainXReshape, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clement' 'Luke' 'Pastoral' 'Paul']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10212029, 0.0048162 , 0.89151037, 0.00155308]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP Results\n",
    "testXReshape = testX.reshape(1,testX.shape[1]*6)\n",
    "print(clf.classes_)\n",
    "clf.predict_proba(testXReshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "['Clement' 'Luke' 'Pastoral' 'Paul']\n",
      "[[0.35007648 0.11171991 0.3611005  0.17710311]]\n"
     ]
    }
   ],
   "source": [
    "#Logisitic Regression results\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(trainXReshape, trainY)\n",
    "print(\"Logistic Regression\")\n",
    "print(clf.classes_)\n",
    "x = clf.predict_proba(testX.reshape(1, -1))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
