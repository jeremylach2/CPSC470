{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from PreprocessLib.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb #allows access to import other notebooks\n",
    "from PreprocessLib import cleanText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loads models\n",
    "longModel = Word2Vec.load(\"longModel.model\")\n",
    "markModel = Word2Vec.load(\"markModel.model\")\n",
    "mathModel = Word2Vec.load(\"mathModel.model\")\n",
    "ignatModel = Word2Vec.load(\"ignatModel.model\")\n",
    "testModel = Word2Vec.load(\"testModel.model\")\n",
    "doc2VecModel = Doc2Vec.load(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a W2V model and returns the associated vocabulary of the model\n",
    "def intoDict (model): \n",
    "    # Get the ordered list of words in the vocabulary\n",
    "    words = model.wv.vocab.keys()\n",
    "    # Make a dictionary\n",
    "    we_dict = {word:model.wv[word] for word in words}\n",
    "    return we_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a dictionary \"dct\", a list of words \"wordList\", and optionally a label\n",
    "#takes each word vector of the words in wordList and adds them to an overall vector, then returns this vector\n",
    "def createVec(dct, wordList, label=\"NA\"):\n",
    "    vec = []\n",
    "    for word in wordList:\n",
    "        vec.append(dct[word])\n",
    "    if (label != \"NA\"):\n",
    "        vec.append(label)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all similar words in the documents\n",
    "def getAllSimWords():\n",
    "    simWords = []\n",
    "    for word in testDict:\n",
    "        if (word in longDict and word in markDict and word in mathDict and word in ignatDict):\n",
    "            simWords.append(word)\n",
    "    return simWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating feature vectors of the models\n",
    "longDict = intoDict(longModel)\n",
    "markDict = intoDict(markModel)\n",
    "mathDict = intoDict(mathModel)\n",
    "ignatDict = intoDict(ignatModel)\n",
    "testDict = intoDict(testModel)\n",
    "    #these words were selected based on results from FrequencyAnalysis.ipynb\n",
    "commonWordList = getAllSimWords()\n",
    "longVec = createVec(longDict, commonWordList, 'Longus')\n",
    "markVec = createVec(markDict, commonWordList, 'Mark')\n",
    "mathVec = createVec(mathDict, commonWordList, 'Mathew')\n",
    "ignatVec = createVec(ignatDict, commonWordList, 'Ignatius')\n",
    "testVec = createVec(testDict, commonWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_doc(a_file):\n",
    "    string_without_line_breaks = \"\"\n",
    "    for line in a_file:\n",
    "        stripped_line = line.rstrip() + \" \"\n",
    "        string_without_line_breaks += stripped_line\n",
    "    a_file.close()\n",
    "    sentences = string_without_line_breaks\n",
    "\n",
    "    cleanedFile = cleanText(sentences, False) #cleanText is found in PreprocessLib.ipynb\n",
    "    return cleanedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "testFile = open(\"./other-sources/Ignatius-3books.txt\", \"r\", encoding=\"utf8\")\n",
    "test = process_file_doc(testFile)\n",
    "longVec.append(doc2VecModel[0])\n",
    "markVec.append(doc2VecModel[1])\n",
    "mathVec.append(doc2VecModel[2])\n",
    "ignatVec.append(doc2VecModel[3])\n",
    "docVecs = {}\n",
    "docVecs['Test'] = doc2VecModel.infer_vector(test.split())\n",
    "testXDoc = np.array(docVecs[\"Test\"])\n",
    "testVec.append(testXDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trainX = np.array((longVec[:len(commonWordList)], markVec[:len(commonWordList)], mathVec[:len(commonWordList)],\n",
    "                   ignatVec[:len(commonWordList)]))\n",
    "trainY =['Longus', 'Mark', \"Mathew\", \"Ignatius\"]\n",
    "testX = np.array(testVec[:len(commonWordList)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns average result of MLP classifier and LogisticRegression classifier iterations times\n",
    "def runIteration(iterations):\n",
    "    av = 0\n",
    "    av2 = 0\n",
    "    for i in range(0, iterations):\n",
    "        nsamples, nx, ny = trainX.shape\n",
    "        trainXReshape = trainX.reshape((nsamples,nx*ny))\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        clf = MLPClassifier(max_iter=8000, hidden_layer_sizes=(4,)).fit(trainXReshape, trainY)\n",
    "        testXReshape = testX.reshape(1,testX.shape[1]*len(commonWordList))\n",
    "        av += clf.predict_proba(testXReshape)[0][0]\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(trainXReshape, trainY)\n",
    "        clf.predict_proba(testX.reshape(1, -1))\n",
    "        av2 += clf.predict_proba(testXReshape)[0][0]\n",
    "    print(\"Average MLP: \", av/iterations)\n",
    "    print(\"Average Log: \", av2/iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MLP:  0.8718874454498291\n",
      "Average Log:  0.8663758617054744\n"
     ]
    }
   ],
   "source": [
    "#vector size 5\n",
    "runIteration(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MLP:  0.9130725175142288\n",
      "Average Log:  0.2956156591941414\n"
     ]
    }
   ],
   "source": [
    "#vector size 100\n",
    "runIteration(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
